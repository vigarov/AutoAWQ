{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AutoAWQ","text":"<p>AutoAWQ pushes ease of use and fast inference speed into one package. In the following documentation, you will learn how to quantize and run inference.</p> <p>Example inference speed (RTX 4090, Ryzen 9 7950X, 64 tokens):</p> <ul> <li>Vicuna 7B (GEMV kernel): 198.848 tokens/s</li> <li>Mistral 7B (GEMM kernel): 156.317 tokens/s</li> <li>Mistral 7B (ExLlamaV2 kernel): 188.865 tokens/s</li> <li>Mixtral 46.7B (GEMM kernel): 93 tokens/s (2x 4090)</li> </ul>"},{"location":"#installation-notes","title":"Installation notes","text":"<ul> <li>Install: <code>pip install autoawq</code>.</li> <li>Your torch version must match the build version, i.e. you cannot use torch 2.0.1 with a wheel that was built with 2.2.0.</li> <li>For AMD GPUs, inference will run through ExLlamaV2 kernels without fused layers. You need to pass the following arguments to run with AMD GPUs:</li> <li>For CPU device, you should install intel-extension-for-transformers with <code>pip install intel-extension-for-transformers</code>. And the latest version of torch is required since \"intel-extension-for-transformers(ITREX)\" was built with the latest version of torch(now ITREX 1.4 was build with torch 2.2). If you build ITREX from source code, then you need to ensure the consistency of the torch version. And you should use \"use_qbits=True\" for CPU device. Up to now, the feature of fuse_layers hasn't been ready for CPU device.</li> </ul> <pre><code>model = AutoAWQForCausalLM.from_quantized(\n    ...,\n    fuse_layers=False,\n    use_exllama_v2=True\n)\n</code></pre>"},{"location":"#supported-models","title":"Supported models","text":"<p>The detailed support list:</p> Models Sizes LLaMA-2 7B/13B/70B LLaMA 7B/13B/30B/65B Mistral 7B Vicuna 7B/13B MPT 7B/30B Falcon 7B/40B OPT 125m/1.3B/2.7B/6.7B/13B/30B Bloom 560m/3B/7B/ GPTJ 6.7B Aquila 7B Aquila2 7B/34B Yi 6B/34B Qwen 1.8B/7B/14B/72B BigCode 1B/7B/15B GPT NeoX 20B GPT-J 6B LLaVa 7B/13B Mixtral 8x7B Baichuan 7B/13B QWen 1.8B/7B/14/72B"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#basic-quantization","title":"Basic Quantization","text":"<p>AWQ performs zero point quantization down to a precision of 4-bit integers. You can also specify other bit rates like 3-bit, but some of these options may lack kernels for running inference.</p> <p>Notes:</p> <ul> <li>Some models like Falcon is only compatible with group size 64.</li> <li>To use Marlin, you must specify zero point as False and version as Marlin.</li> </ul> <pre><code>from awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer\n\nmodel_path = 'mistralai/Mistral-7B-Instruct-v0.2'\nquant_path = 'mistral-instruct-v0.2-awq'\nquant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n\n# Load model\nmodel = AutoAWQForCausalLM.from_pretrained(\n    model_path, **{\"low_cpu_mem_usage\": True, \"use_cache\": False}\n)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\n# Quantize\nmodel.quantize(tokenizer, quant_config=quant_config)\n\n# Save quantized model\nmodel.save_quantized(quant_path)\ntokenizer.save_pretrained(quant_path)\n\nprint(f'Model is quantized and saved at \"{quant_path}\"')\n</code></pre>"},{"location":"examples/#custom-data","title":"Custom Data","text":"<p>This includes an example function that loads either wikitext or dolly. Note that currently all samples above 512 in length are discarded.</p> <pre><code>from datasets import load_dataset\nfrom awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer\n\nmodel_path = 'lmsys/vicuna-7b-v1.5'\nquant_path = 'vicuna-7b-v1.5-awq'\nquant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n\n# Load model\nmodel = AutoAWQForCausalLM.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\n# Define data loading methods\ndef load_dolly():\n    data = load_dataset('databricks/databricks-dolly-15k', split=\"train\")\n\n    # concatenate data\n    def concatenate_data(x):\n        return {\"text\": x['instruction'] + '\\n' + x['context'] + '\\n' + x['response']}\n\n    concatenated = data.map(concatenate_data)\n    return [text for text in concatenated[\"text\"]]\n\ndef load_wikitext():\n    data = load_dataset('wikitext', 'wikitext-2-raw-v1', split=\"train\")\n    return [text for text in data[\"text\"] if text.strip() != '' and len(text.split(' ')) &gt; 20]\n\n# Quantize\nmodel.quantize(tokenizer, quant_config=quant_config, calib_data=load_wikitext())\n\n# Save quantized model\nmodel.save_quantized(quant_path)\ntokenizer.save_pretrained(quant_path)\n\nprint(f'Model is quantized and saved at \"{quant_path}\"')\n</code></pre>"},{"location":"examples/#gguf-export","title":"GGUF Export","text":"<p>This computes AWQ scales and appliesthem to the model without running real quantization. This keeps the quality of AWQ because theweights are applied but skips quantization in order to make it compatible with other frameworks.</p> <p>Step by step:</p> <ul> <li><code>quantize()</code>: Compute AWQ scales and apply them</li> <li><code>save_pretrained()</code>: Saves a non-quantized model in FP16</li> <li><code>convert.py</code>: Convert the Huggingface FP16 weights to GGUF FP16 weights</li> <li><code>quantize</code>: Run GGUF quantization to get real quantized weights, in this case 4-bit.</li> </ul> <pre><code>import os\nimport subprocess\nfrom awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer\n\nmodel_path = 'mistralai/Mistral-7B-v0.1'\nquant_path = 'mistral-awq'\nllama_cpp_path = '/workspace/llama.cpp'\nquant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 6, \"version\": \"GEMM\" }\n\n# Load model\n# NOTE: pass safetensors=True to load safetensors\nmodel = AutoAWQForCausalLM.from_pretrained(\n    model_path, **{\"low_cpu_mem_usage\": True, \"use_cache\": False}\n)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\n# Quantize\n# NOTE: We avoid packing weights, so you cannot use this model in AutoAWQ\n# after quantizing. The saved model is FP16 but has the AWQ scales applied.\nmodel.quantize(\n    tokenizer,\n    quant_config=quant_config,\n    export_compatible=True\n)\n\n# Save quantized model\nmodel.save_quantized(quant_path)\ntokenizer.save_pretrained(quant_path)\nprint(f'Model is quantized and saved at \"{quant_path}\"')\n\n# GGUF conversion\nprint('Converting model to GGUF...')\nllama_cpp_method = \"q4_K_M\"\nconvert_cmd_path = os.path.join(llama_cpp_path, \"convert.py\")\nquantize_cmd_path = os.path.join(llama_cpp_path, \"quantize\")\n\nif not os.path.exists(llama_cpp_path):\n    cmd = f\"git clone https://github.com/ggerganov/llama.cpp.git {llama_cpp_path} &amp;&amp; cd {llama_cpp_path} &amp;&amp; make LLAMA_CUBLAS=1 LLAMA_CUDA_F16=1\"\n    subprocess.run([cmd], shell=True, check=True)\n\nsubprocess.run([\n    f\"python {convert_cmd_path} {quant_path} --outfile {quant_path}/model.gguf\"\n], shell=True, check=True)\n\nsubprocess.run([\n    f\"{quantize_cmd_path} {quant_path}/model.gguf {quant_path}/model_{llama_cpp_method}.gguf {llama_cpp_method}\"\n], shell=True, check=True)\n</code></pre>"},{"location":"examples/#basic-inference","title":"Basic Inference","text":""},{"location":"examples/#inference-with-gpu","title":"Inference With GPU","text":"<p>To run inference, you often want to run with <code>fuse_layers=True</code> to get the claimed speedup in AutoAWQ. Additionally, consider setting <code>max_seq_len</code> (default: 2048) as this will be the maximum context that the model can hold.</p> <p>Notes:</p> <ul> <li>You can specify <code>use_exllama_v2=True</code> to enable ExLlamaV2 kernels during inference.</li> </ul> <pre><code>from awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer, TextStreamer\n\nquant_path = \"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\"\n\n# Load model\nmodel = AutoAWQForCausalLM.from_quantized(quant_path, fuse_layers=True)\ntokenizer = AutoTokenizer.from_pretrained(quant_path, trust_remote_code=True)\nstreamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n\n# Convert prompt to tokens\nprompt_template = \"[INST] {prompt} [/INST]\"\n\nprompt = \"You're standing on the surface of the Earth. \"\\\n        \"You walk one mile south, one mile west and one mile north. \"\\\n        \"You end up exactly where you started. Where are you?\"\n\ntokens = tokenizer(\n    prompt_template.format(prompt=prompt), \n    return_tensors='pt'\n).input_ids.cuda()\n\n# Generate output\ngeneration_output = model.generate(\n    tokens, \n    streamer=streamer,\n    max_new_tokens=512\n)\n</code></pre>"},{"location":"examples/#inference-with-cpu","title":"Inference With CPU","text":"<p>To run inference with CPU , you should specify <code>use_qbits=True</code>. QBits is the backend for CPU including kernel for operators. QBits is a module of the intel-extension-for-transformers package. Up to now, the feature of fusing layers hasn't been ready, you should run model with <code>fuse_layers=False</code>.</p> <pre><code>from awq import AutoAWQForCausalLM\n\nquant_path = \"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\"\n# Load model\nmodel = AutoAWQForCausalLM.from_quantized(quant_path, fuse_layers=False, use_qbits=True)\n</code></pre>"},{"location":"examples/#transformers","title":"Transformers","text":"<p>You can also load an AWQ model by using AutoModelForCausalLM, just make sure you have AutoAWQ installed. Note that not all models will have fused modules when loading from transformers. See more documentation here.</p> <pre><code>import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n\n# NOTE: Must install from PR until merged\n# pip install --upgrade git+https://github.com/younesbelkada/transformers.git@add-awq\nmodel_id = \"casperhansen/mistral-7b-instruct-v0.1-awq\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id, \n    torch_dtype=torch.float16, \n    low_cpu_mem_usage=True,\n    device_map=\"cuda:0\"\n)\nstreamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n\n# Convert prompt to tokens\ntext = \"[INST] What are the basic steps to use the Huggingface transformers library? [/INST]\"\n\ntokens = tokenizer(\n    text, \n    return_tensors='pt'\n).input_ids.cuda()\n\n# Generate output\ngeneration_output = model.generate(\n    tokens, \n    streamer=streamer,\n    max_new_tokens=512\n)\n</code></pre>"},{"location":"examples/#vllm","title":"vLLM","text":"<p>You can also load AWQ models in vLLM.</p> <pre><code>import asyncio\nfrom transformers import AutoTokenizer, PreTrainedTokenizer\nfrom vllm import AsyncLLMEngine, SamplingParams, AsyncEngineArgs\n\nmodel_path = \"casperhansen/mixtral-instruct-awq\"\n\n# prompting\nprompt = \"You're standing on the surface of the Earth. \"\\\n         \"You walk one mile south, one mile west and one mile north. \"\\\n         \"You end up exactly where you started. Where are you?\",\n\nprompt_template = \"[INST] {prompt} [/INST]\"\n\n# sampling params\nsampling_params = SamplingParams(\n    repetition_penalty=1.1,\n    temperature=0.8,\n    max_tokens=512\n)\n\n# tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# async engine args for streaming\nengine_args = AsyncEngineArgs(\n    model=model_path,\n    quantization=\"awq\",\n    dtype=\"float16\",\n    max_model_len=512,\n    enforce_eager=True,\n    disable_log_requests=True,\n    disable_log_stats=True,\n)\n\nasync def generate(model: AsyncLLMEngine, tokenizer: PreTrainedTokenizer):\n    tokens = tokenizer(prompt_template.format(prompt=prompt)).input_ids\n\n    outputs = model.generate(\n        prompt=prompt,\n        sampling_params=sampling_params,\n        request_id=1,\n        prompt_token_ids=tokens,\n    )\n\n    print(\"\\n** Starting generation!\\n\")\n    last_index = 0\n\n    async for output in outputs:\n        print(output.outputs[0].text[last_index:], end=\"\", flush=True)\n        last_index = len(output.outputs[0].text)\n\n    print(\"\\n\\n** Finished generation!\\n\")\n\nif __name__ == '__main__':\n    model = AsyncLLMEngine.from_engine_args(engine_args)\n    asyncio.run(generate(model, tokenizer))\n</code></pre>"},{"location":"examples/#llava-multimodal","title":"LLaVa (multimodal)","text":"<p>AutoAWQ also supports the LLaVa model. You simply need to load an  AutoProcessor to process the prompt and image to generate inputs for the AWQ model.</p> <pre><code>import requests\nimport torch\nfrom PIL import Image\n\nfrom awq import AutoAWQForCausalLM\nfrom transformers import AutoProcessor\n\nquant_path = \"ybelkada/llava-1.5-7b-hf-awq\"\n\n# Load model\nmodel = AutoAWQForCausalLM.from_quantized(quant_path, safetensors=True, device_map={\"\": 0})\nprocessor = AutoProcessor.from_pretrained(quant_path)\n\nprompt = \"USER: &lt;image&gt;\\nWhat are these?\\nASSISTANT:\"\nimage_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n\nraw_image = Image.open(requests.get(image_file, stream=True).raw)\ninputs = processor(prompt, raw_image, return_tensors='pt').to(0, torch.float16)\n# Generate output\ngeneration_output = model.generate(\n    **inputs, \n    max_new_tokens=512\n)\n\nprint(processor.decode(generation_output[0], skip_special_tokens=True))\n</code></pre>"},{"location":"reference/","title":"Auto and Base model classes in AutoAWQ","text":"<p>View the documentation of the main classes of AutoAWQ models below.</p>"},{"location":"reference/#awq.models.auto.AutoAWQForCausalLM","title":"awq.models.auto.AutoAWQForCausalLM","text":"<pre><code>AutoAWQForCausalLM()\n</code></pre> Source code in <code>awq/models/auto.py</code> <pre><code>def __init__(self):\n    raise EnvironmentError(\n        \"You must instantiate AutoAWQForCausalLM with\\n\"\n        \"AutoAWQForCausalLM.from_quantized or AutoAWQForCausalLM.from_pretrained\"\n    )\n</code></pre>"},{"location":"reference/#awq.models.auto.AutoAWQForCausalLM.from_pretrained","title":"from_pretrained  <code>classmethod</code>","text":"<pre><code>from_pretrained(model_path, trust_remote_code=True, safetensors=True, device_map=None, download_kwargs=None, **model_init_kwargs)\n</code></pre> PARAMETER DESCRIPTION <code>model_path</code> <code>trust_remote_code</code> <p> DEFAULT: <code>True</code> </p> <code>safetensors</code> <p> DEFAULT: <code>True</code> </p> <code>device_map</code> <p> DEFAULT: <code>None</code> </p> <code>download_kwargs</code> <p> DEFAULT: <code>None</code> </p> <code>**model_init_kwargs</code> <p> DEFAULT: <code>{}</code> </p> Source code in <code>awq/models/auto.py</code> <pre><code>@classmethod\ndef from_pretrained(\n    self,\n    model_path,\n    trust_remote_code=True,\n    safetensors=True,\n    device_map=None,\n    download_kwargs=None,\n    **model_init_kwargs,\n) -&gt; BaseAWQForCausalLM:\n    model_type = check_and_get_model_type(\n        model_path, trust_remote_code, **model_init_kwargs\n    )\n\n    return AWQ_CAUSAL_LM_MODEL_MAP[model_type].from_pretrained(\n        model_path,\n        model_type,\n        trust_remote_code=trust_remote_code,\n        safetensors=safetensors,\n        device_map=device_map,\n        download_kwargs=download_kwargs,\n        **model_init_kwargs,\n    )\n</code></pre>"},{"location":"reference/#awq.models.auto.AutoAWQForCausalLM.from_quantized","title":"from_quantized  <code>classmethod</code>","text":"<pre><code>from_quantized(quant_path, quant_filename='', max_seq_len=2048, trust_remote_code=True, fuse_layers=True, use_exllama=False, use_exllama_v2=False, use_qbits=False, batch_size=1, safetensors=True, device_map='balanced', max_memory=None, offload_folder=None, download_kwargs=None, **config_kwargs)\n</code></pre> PARAMETER DESCRIPTION <code>quant_path</code> <code>quant_filename</code> <p> DEFAULT: <code>''</code> </p> <code>max_seq_len</code> <p> DEFAULT: <code>2048</code> </p> <code>trust_remote_code</code> <p> DEFAULT: <code>True</code> </p> <code>fuse_layers</code> <p> DEFAULT: <code>True</code> </p> <code>use_exllama</code> <p> DEFAULT: <code>False</code> </p> <code>use_exllama_v2</code> <p> DEFAULT: <code>False</code> </p> <code>use_qbits</code> <p> DEFAULT: <code>False</code> </p> <code>batch_size</code> <p> DEFAULT: <code>1</code> </p> <code>safetensors</code> <p> DEFAULT: <code>True</code> </p> <code>device_map</code> <p> DEFAULT: <code>'balanced'</code> </p> <code>max_memory</code> <p> DEFAULT: <code>None</code> </p> <code>offload_folder</code> <p> DEFAULT: <code>None</code> </p> <code>download_kwargs</code> <p> DEFAULT: <code>None</code> </p> <code>**config_kwargs</code> <p> DEFAULT: <code>{}</code> </p> Source code in <code>awq/models/auto.py</code> <pre><code>@classmethod\ndef from_quantized(\n    self,\n    quant_path,\n    quant_filename=\"\",\n    max_seq_len=2048,\n    trust_remote_code=True,\n    fuse_layers=True,\n    use_exllama=False,\n    use_exllama_v2=False,\n    use_qbits=False,\n    batch_size=1,\n    safetensors=True,\n    device_map=\"balanced\",\n    max_memory=None,\n    offload_folder=None,\n    download_kwargs=None,\n    **config_kwargs,\n) -&gt; BaseAWQForCausalLM:\n    os.environ[\"AWQ_BATCH_SIZE\"] = str(batch_size)\n    model_type = check_and_get_model_type(quant_path, trust_remote_code)\n\n    if config_kwargs.get(\"max_new_tokens\") is not None:\n        max_seq_len = config_kwargs[\"max_new_tokens\"]\n        logging.warning(\n            \"max_new_tokens argument is deprecated... gracefully \"\n            \"setting max_seq_len=max_new_tokens.\"\n        )\n\n    return AWQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized(\n        quant_path,\n        model_type,\n        quant_filename,\n        max_seq_len,\n        trust_remote_code=trust_remote_code,\n        fuse_layers=fuse_layers,\n        use_exllama=use_exllama,\n        use_exllama_v2=use_exllama_v2,\n        use_qbits=use_qbits,\n        safetensors=safetensors,\n        device_map=device_map,\n        max_memory=max_memory,\n        offload_folder=offload_folder,\n        download_kwargs=download_kwargs,\n        **config_kwargs,\n    )\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM","title":"awq.models.base.BaseAWQForCausalLM","text":"<pre><code>BaseAWQForCausalLM(model, model_type, is_quantized, config, quant_config, processor)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>The base model for all AutoAWQ models.</p> PARAMETER DESCRIPTION <code>model</code> <p>The pretrained or quantized model.</p> <p> TYPE: <code>PreTrainedModel</code> </p> <code>model_type</code> <p>The model type, found in config.json.</p> <p> TYPE: <code>str</code> </p> <code>is_quantized</code> <p>Indicates if the current model is quantized.</p> <p> TYPE: <code>bool</code> </p> <code>config</code> <p>The config of the model.</p> <p> TYPE: <code>PretrainedConfig</code> </p> <code>quant_config</code> <p>The quantization config of the model.</p> <p> TYPE: <code>AwqConfig</code> </p> <code>processor</code> <p>An optional processor, e.g. for vision models.</p> <p> TYPE: <code>AutoProcessor</code> </p> Source code in <code>awq/models/base.py</code> <pre><code>def __init__(\n    self,\n    model: Annotated[PreTrainedModel, Doc(\"The pretrained or quantized model.\")],\n    model_type: Annotated[str, Doc(\"The model type, found in config.json.\")],\n    is_quantized: Annotated[\n        bool, Doc(\"Indicates if the current model is quantized.\")\n    ],\n    config: Annotated[PretrainedConfig, Doc(\"The config of the model.\")],\n    quant_config: Annotated[\n        AwqConfig, Doc(\"The quantization config of the model.\")\n    ],\n    processor: Annotated[\n        AutoProcessor, Doc(\"An optional processor, e.g. for vision models.\")\n    ],\n):\n    \"\"\"The base model for all AutoAWQ models.\"\"\"\n    super().__init__()\n    self.model: PreTrainedModel = model\n    self.model_type: str = model_type\n    self.is_quantized: bool = is_quantized\n    self.search_result = None\n    self.config: PretrainedConfig = config\n    self.quant_config: AwqConfig = quant_config\n    self.processor: CLIPImageProcessor = processor\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = model\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.model_type","title":"model_type  <code>instance-attribute</code>","text":"<pre><code>model_type = model_type\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.is_quantized","title":"is_quantized  <code>instance-attribute</code>","text":"<pre><code>is_quantized = is_quantized\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.search_result","title":"search_result  <code>instance-attribute</code>","text":"<pre><code>search_result = None\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.quant_config","title":"quant_config  <code>instance-attribute</code>","text":"<pre><code>quant_config = quant_config\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.processor","title":"processor  <code>instance-attribute</code>","text":"<pre><code>processor = processor\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.to","title":"to","text":"<pre><code>to(device)\n</code></pre> <p>A utility function for moving the model to a device.</p> PARAMETER DESCRIPTION <code>device</code> <p>The device to move your model to.</p> <p> TYPE: <code>str</code> </p> Source code in <code>awq/models/base.py</code> <pre><code>def to(self, device: Annotated[str, Doc(\"The device to move your model to.\")]):\n    \"\"\"A utility function for moving the model to a device.\"\"\"\n    return self.model.to(device)\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.forward","title":"forward","text":"<pre><code>forward(*args, **kwargs)\n</code></pre> <p>A forward function that mimics the torch forward.</p> PARAMETER DESCRIPTION <code>*args</code> <p> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p> DEFAULT: <code>{}</code> </p> Source code in <code>awq/models/base.py</code> <pre><code>def forward(self, *args, **kwargs):\n    \"\"\"A forward function that mimics the torch forward.\"\"\"\n    return self.model(*args, **kwargs)\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.generate","title":"generate","text":"<pre><code>generate(*args, **kwargs)\n</code></pre> <p>A generate function that mimics the HF generate function.</p> PARAMETER DESCRIPTION <code>*args</code> <p> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p> DEFAULT: <code>{}</code> </p> Source code in <code>awq/models/base.py</code> <pre><code>def generate(self, *args, **kwargs):\n    \"\"\"A generate function that mimics the HF generate function.\"\"\"\n    with torch.inference_mode():\n        return self.model.generate(*args, **kwargs)\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.quantize","title":"quantize","text":"<pre><code>quantize(tokenizer=None, quant_config={}, calib_data='pileval', split='train', text_column='text', duo_scaling=True, export_compatible=False, apply_clip=True)\n</code></pre> <p>The main quantization function that you can use to quantize your model.</p> <p>Example:</p> <pre><code>from awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer\n\nmodel_path = \"...\"\nmodel = AutoAWQForCausalLM.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\nquant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\nmodel.quantize(tokenizer, quant_config)\n</code></pre> PARAMETER DESCRIPTION <code>tokenizer</code> <p>The tokenizer to use for quantization.</p> <p> TYPE: <code>PreTrainedTokenizer</code> DEFAULT: <code>None</code> </p> <code>quant_config</code> <p>The quantization config you want to use.</p> <p> TYPE: <code>Dict</code> DEFAULT: <code>{}</code> </p> <code>calib_data</code> <p>The calibration dataset. Either a string pointing to Huggingface or a list of preloaded examples.</p> <p> TYPE: <code>Union[str, List[str]]</code> DEFAULT: <code>'pileval'</code> </p> <code>split</code> <p>The split of calib_data.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'train'</code> </p> <code>text_column</code> <p>The text column of calib_data.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'text'</code> </p> <code>duo_scaling</code> <p>Whether to scale using both w/x or just x.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>export_compatible</code> <p>This argument avoids real quantization by only applying the scales without quantizing down to FP16.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>apply_clip</code> <p>Whether to apply clipping to the model during quantization. Some models may perform better with this set to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>awq/models/base.py</code> <pre><code>@torch.no_grad()\ndef quantize(\n    self,\n    tokenizer: Annotated[\n        PreTrainedTokenizer, Doc(\"The tokenizer to use for quantization.\")\n    ] = None,\n    quant_config: Annotated[\n        Dict, Doc(\"The quantization config you want to use.\")\n    ] = {},\n    calib_data: Annotated[\n        Union[str, List[str]],\n        Doc(\n            \"The calibration dataset. Either a string pointing to Huggingface or a list of preloaded examples.\"\n        ),\n    ] = \"pileval\",\n    split: Annotated[str, Doc(\"The split of calib_data.\")] = \"train\",\n    text_column: Annotated[str, Doc(\"The text column of calib_data.\")] = \"text\",\n    duo_scaling: Annotated[\n        bool, Doc(\"Whether to scale using both w/x or just x.\")\n    ] = True,\n    export_compatible: Annotated[\n        bool,\n        Doc(\n            \"This argument avoids real quantization by only applying the scales without quantizing down to FP16.\"\n        ),\n    ] = False,\n    apply_clip: Annotated[\n        bool,\n        Doc(\n            \"Whether to apply clipping to the model during quantization. Some models may perform better with this set to False.\"\n        ),\n    ] = True,\n):\n    \"\"\"\n    The main quantization function that you can use to quantize your model.\n\n    Example:\n\n    ```python\n    from awq import AutoAWQForCausalLM\n    from transformers import AutoTokenizer\n\n    model_path = \"...\"\n    model = AutoAWQForCausalLM.from_pretrained(model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n    model.quantize(tokenizer, quant_config)\n    ```\n    \"\"\"\n    self.quant_config: AwqConfig = AwqConfig.from_dict(quant_config)\n\n    if hasattr(self, \"modules_to_not_convert\"):\n        self.quant_config.modules_to_not_convert = self.modules_to_not_convert\n\n    self.quantizer = AwqQuantizer(\n        self,\n        self.model,\n        tokenizer,\n        self.quant_config.w_bit,\n        self.quant_config.q_group_size,\n        self.quant_config.zero_point,\n        self.quant_config.version,\n        calib_data,\n        split,\n        text_column,\n        duo_scaling,\n        modules_to_not_convert=self.quant_config.modules_to_not_convert,\n        export_compatible=export_compatible,\n        apply_clip=apply_clip,\n    )\n    self.quantizer.quantize()\n\n    self.is_quantized = True\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.pack","title":"pack","text":"<pre><code>pack()\n</code></pre> <p>A utility function for the following scenario. Note that save_quantized will overwrite existing weights if you use the same quant_path.</p> <p>Example:</p> <pre><code>model.quantize(\n    tokenizer,\n    quant_config=quant_config,\n    export_compatible=True\n)\nmodel.save_quantized(...)  # produces GGUF/other compat weights\nmodel.pack(...) # makes the model CUDA compat\nmodel.save_quantized(...)  # produces CUDA compat weights\n</code></pre> Source code in <code>awq/models/base.py</code> <pre><code>@torch.no_grad()\ndef pack(self):\n    \"\"\"\n    A utility function for the following scenario. Note that save_quantized will\n    overwrite existing weights if you use the same quant_path.\n\n    Example:\n\n    ```python\n    model.quantize(\n        tokenizer,\n        quant_config=quant_config,\n        export_compatible=True\n    )\n    model.save_quantized(...)  # produces GGUF/other compat weights\n    model.pack(...) # makes the model CUDA compat\n    model.save_quantized(...)  # produces CUDA compat weights\n    ```\n    \"\"\"\n    self.quantizer.pack()\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.fuse_layers","title":"fuse_layers  <code>staticmethod</code>","text":"<pre><code>fuse_layers(model)\n</code></pre> PARAMETER DESCRIPTION <code>model</code> Source code in <code>awq/models/base.py</code> <pre><code>@staticmethod\ndef fuse_layers(model):\n    pass\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.save_quantized","title":"save_quantized","text":"<pre><code>save_quantized(save_dir, safetensors=True, shard_size='5GB')\n</code></pre> PARAMETER DESCRIPTION <code>save_dir</code> <p>The directory to save your model to.</p> <p> TYPE: <code>str</code> </p> <code>safetensors</code> <p>Whether to save the model as safetensors or torch files.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>shard_size</code> <p>The shard size for sharding large models into multiple chunks.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'5GB'</code> </p> Source code in <code>awq/models/base.py</code> <pre><code>def save_quantized(\n    self,\n    save_dir: Annotated[str, Doc(\"The directory to save your model to.\")],\n    safetensors: Annotated[\n        bool, Doc(\"Whether to save the model as safetensors or torch files.\")\n    ] = True,\n    shard_size: Annotated[\n        str, Doc(\"The shard size for sharding large models into multiple chunks.\")\n    ] = \"5GB\",\n):\n    save_dir = save_dir[:-1] if save_dir[-1] == \"/\" else save_dir\n\n    # Save model\n    class EmptyModule(nn.Module):\n        def __init__(self):\n            super(EmptyModule, self).__init__()\n\n        def forward(self, x):\n            return x\n\n    # Save model and config files with empty state dict\n    self.model.config.quantization_config = self.quant_config.to_transformers_dict()\n    self.model.generation_config.do_sample = True\n    self.model.save_pretrained(save_dir, state_dict=EmptyModule().state_dict())\n\n    # Vision transformers have a processor\n    if self.processor is not None:\n        self.processor.save_pretrained(save_dir)\n\n    # Remove empty state dict\n    default_paths = [\n        f\"{save_dir}/model.safetensors\",\n        f\"{save_dir}/pytorch_model.bin\",\n    ]\n    for path in default_paths:\n        if os.path.exists(path):\n            os.remove(path)\n\n    # model_name has no extension, add it when saving state_dict\n    model_name = \"model.safetensors\" if safetensors else \"pytorch_model.bin\"\n\n    # shard checkpoint into chunks (10GB default)\n    shards, index = shard_checkpoint(\n        self.model.state_dict(), max_shard_size=shard_size, weights_name=model_name\n    )\n\n    for shard_file, shard in shards.items():\n        if safetensors:\n            # safetensors must be in the same memory, so we duplicate and use contiguous memory\n            shard = {k: v.clone().contiguous() for k, v in shard.items()}\n            save_file(\n                shard, os.path.join(save_dir, shard_file), metadata={\"format\": \"pt\"}\n            )\n        else:\n            torch.save(shard, os.path.join(save_dir, shard_file))\n\n    # save shard index\n    if index is not None:\n        with open(f\"{save_dir}/{model_name}.index.json\", \"w+\") as file:\n            file.write(json.dumps(index, indent=4))\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.from_pretrained","title":"from_pretrained  <code>classmethod</code>","text":"<pre><code>from_pretrained(model_path, model_type, torch_dtype=torch.float16, trust_remote_code=True, safetensors=True, device_map=None, download_kwargs=None, **model_init_kwargs)\n</code></pre> <p>A method for initialization of pretrained models, usually in FP16.</p> PARAMETER DESCRIPTION <code>model_path</code> <p>A Huggingface path or local path to a model.</p> <p> TYPE: <code>str</code> </p> <code>model_type</code> <p>The model type, loaded from config.json.</p> <p> TYPE: <code>str</code> </p> <code>torch_dtype</code> <p>The dtype to load the model as. May not work with other values than float16.</p> <p> TYPE: <code>dtype</code> DEFAULT: <code>float16</code> </p> <code>trust_remote_code</code> <p>Useful for Huggingface repositories that have not been integrated into transformers yet.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>safetensors</code> <p>Whether to download/load safetensors instead of torch weights.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>device_map</code> <p>A device map that will be passed onto the model loading method from transformers.</p> <p> TYPE: <code>Union[str, Dict]</code> DEFAULT: <code>None</code> </p> <code>download_kwargs</code> <p>Used for configure download model</p> <p> TYPE: <code>Dict</code> DEFAULT: <code>None</code> </p> <code>**model_init_kwargs</code> <p>Additional kwargs that are passed to the model during initialization.</p> <p> TYPE: <code>Dict</code> DEFAULT: <code>{}</code> </p> Source code in <code>awq/models/base.py</code> <pre><code>@classmethod\ndef from_pretrained(\n    self,\n    model_path: Annotated[str, Doc(\"A Huggingface path or local path to a model.\")],\n    model_type: Annotated[str, Doc(\"The model type, loaded from config.json.\")],\n    torch_dtype: Annotated[\n        torch.dtype,\n        Doc(\n            \"The dtype to load the model as. May not work with other values than float16.\"\n        ),\n    ] = torch.float16,\n    trust_remote_code: Annotated[\n        bool,\n        Doc(\n            \"Useful for Huggingface repositories that have not been integrated into transformers yet.\"\n        ),\n    ] = True,\n    safetensors: Annotated[\n        bool, Doc(\"Whether to download/load safetensors instead of torch weights.\")\n    ] = True,\n    device_map: Annotated[\n        Union[str, Dict],\n        Doc(\n            \"A device map that will be passed onto the model loading method from transformers.\"\n        ),\n    ] = None,\n    download_kwargs: Annotated[\n        Dict, Doc(\"Used for configure download model\"),\n    ] = None,\n    **model_init_kwargs: Annotated[\n        Dict,\n        Doc(\n            \"Additional kwargs that are passed to the model during initialization.\"\n        ),\n    ],\n):\n    \"\"\"A method for initialization of pretrained models, usually in FP16.\"\"\"\n    # Get weights path and quant config\n    model_weights_path, config, quant_config = self._load_config(\n        self, model_path, \"\", safetensors,\n        trust_remote_code=trust_remote_code,\n        download_kwargs=download_kwargs\n    )\n\n    target_cls_name = TRANSFORMERS_AUTO_MAPPING_DICT[config.model_type]\n    target_cls = getattr(transformers, target_cls_name)\n\n    processor = None\n    if target_cls_name == \"AutoModelForVision2Seq\":\n        processor = AutoProcessor.from_pretrained(model_weights_path)\n        processor: CLIPImageProcessor = processor.image_processor\n\n    # If not quantized, must load with AutoModelForCausalLM\n    model = target_cls.from_pretrained(\n        model_weights_path,\n        trust_remote_code=trust_remote_code,\n        torch_dtype=torch_dtype,\n        use_safetensors=safetensors,\n        device_map=device_map,\n        **model_init_kwargs,\n    )\n\n    model.eval()\n\n    return self(\n        model,\n        model_type,\n        is_quantized=False,\n        config=config,\n        quant_config=quant_config,\n        processor=processor,\n    )\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.from_quantized","title":"from_quantized  <code>classmethod</code>","text":"<pre><code>from_quantized(model_path, model_type, model_filename='', max_seq_len=None, torch_dtype=torch.float16, trust_remote_code=True, safetensors=True, fuse_layers=True, use_exllama=False, use_exllama_v2=False, use_qbits=False, device_map='balanced', max_memory=None, offload_folder=None, download_kwargs=None, **config_kwargs)\n</code></pre> <p>A method for initialization of a quantized model, usually in INT4.</p> PARAMETER DESCRIPTION <code>model_path</code> <p>A Huggingface path or local path to a model.</p> <p> TYPE: <code>str</code> </p> <code>model_type</code> <p>The model type, loaded from config.json.</p> <p> TYPE: <code>str</code> </p> <code>model_filename</code> <p>Load a specific model's filename by specifying this argument.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>max_seq_len</code> <p>The maximum sequence cached sequence length of the model. Larger values may increase loading time and memory usage.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>torch_dtype</code> <p>The dtype to load the model as. May not work with other values than float16.</p> <p> TYPE: <code>dtype</code> DEFAULT: <code>float16</code> </p> <code>trust_remote_code</code> <p>Useful for Huggingface repositories that have not been integrated into transformers yet.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>safetensors</code> <p>Whether to download/load safetensors instead of torch weights.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>fuse_layers</code> <p>Whether to use fused/optimized combination of layers for increased speed.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>use_exllama</code> <p>Whether to map the weights to ExLlamaV1 kernels.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>use_exllama_v2</code> <p>Whether to map the weights to ExLlamaV2 kernels.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>use_qbits</code> <p>Whether to map the weights to qbits kernels for CPU device.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>device_map</code> <p>A device map that will be passed onto the model loading method from transformers.</p> <p> TYPE: <code>Union[str, Dict]</code> DEFAULT: <code>'balanced'</code> </p> <code>max_memory</code> <p>A dictionary device identifier to maximum memory which will be passed onto the model loading method from transformers. For example\uff1a{0: \"4GB\",1: \"10GB\"</p> <p> TYPE: <code>Dict[Union[int, str], Union[int, str]]</code> DEFAULT: <code>None</code> </p> <code>offload_folder</code> <p>The folder ot offload the model to.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>download_kwargs</code> <p>Used for configure download model</p> <p> TYPE: <code>Dict</code> DEFAULT: <code>None</code> </p> <code>**config_kwargs</code> <p>Additional kwargs that are passed to the config during initialization.</p> <p> TYPE: <code>Dict</code> DEFAULT: <code>{}</code> </p> Source code in <code>awq/models/base.py</code> <pre><code>@classmethod\ndef from_quantized(\n    self,\n    model_path: Annotated[str, Doc(\"A Huggingface path or local path to a model.\")],\n    model_type: Annotated[str, Doc(\"The model type, loaded from config.json.\")],\n    model_filename: Annotated[\n        str, Doc(\"Load a specific model's filename by specifying this argument.\")\n    ] = \"\",\n    max_seq_len: Annotated[\n        int,\n        Doc(\n            \"The maximum sequence cached sequence length of the model. Larger values may increase loading time and memory usage.\"\n        ),\n    ] = None,\n    torch_dtype: Annotated[\n        torch.dtype,\n        Doc(\n            \"The dtype to load the model as. May not work with other values than float16.\"\n        ),\n    ] = torch.float16,\n    trust_remote_code: Annotated[\n        bool,\n        Doc(\n            \"Useful for Huggingface repositories that have not been integrated into transformers yet.\"\n        ),\n    ] = True,\n    safetensors: Annotated[\n        bool, Doc(\"Whether to download/load safetensors instead of torch weights.\")\n    ] = True,\n    fuse_layers: Annotated[\n        bool,\n        Doc(\n            \"Whether to use fused/optimized combination of layers for increased speed.\"\n        ),\n    ] = True,\n    use_exllama: Annotated[\n        bool, Doc(\"Whether to map the weights to ExLlamaV1 kernels.\")\n    ] = False,\n    use_exllama_v2: Annotated[\n        bool, Doc(\"Whether to map the weights to ExLlamaV2 kernels.\")\n    ] = False,\n    use_qbits: Annotated[\n        bool, Doc(\"Whether to map the weights to qbits kernels for CPU device.\")\n    ] = False,\n    device_map: Annotated[\n        Union[str, Dict],\n        Doc(\n            \"A device map that will be passed onto the model loading method from transformers.\"\n        ),\n    ] = \"balanced\",\n    max_memory: Annotated[\n        Dict[Union[int, str], Union[int, str]], \n        Doc(\n            'A dictionary device identifier to maximum memory which will be passed onto the model loading method from transformers. For example\uff1a{0: \"4GB\",1: \"10GB\"'\n        ),\n    ] = None,\n    offload_folder: Annotated[\n        str,\n        Doc(\"The folder ot offload the model to.\"),\n    ] = None,\n    download_kwargs: Annotated[\n        Dict, Doc(\"Used for configure download model\"),\n    ] = None,\n    **config_kwargs: Annotated[\n        Dict,\n        Doc(\n            \"Additional kwargs that are passed to the config during initialization.\"\n        ),\n    ],\n):\n    \"\"\"A method for initialization of a quantized model, usually in INT4.\"\"\"\n    # [STEP 1-2] Load weights path and configs\n    model_weights_path, config, quant_config = self._load_config(\n        self,\n        model_path,\n        model_filename,\n        safetensors,\n        trust_remote_code,\n        max_seq_len=max_seq_len,\n        download_kwargs=download_kwargs,\n        **config_kwargs,\n    )\n\n    target_cls_name = TRANSFORMERS_AUTO_MAPPING_DICT[config.model_type]\n    target_cls = getattr(transformers, target_cls_name)\n\n    # [STEP 3] Load model\n    with init_empty_weights():\n        model = target_cls.from_config(\n            config=config,\n            torch_dtype=torch_dtype,\n            trust_remote_code=trust_remote_code,\n        )\n\n    use_cpu_qbits = use_qbits or get_best_device() == \"cpu\"\n    if use_cpu_qbits:\n        if not qbits_available:\n            raise ImportError(\"Please install intel-extension-for-transformers with \"\n                              \"`pip install intel-extension-for-transformers` for 'qbits' kernel!\")\n\n        fuse_layers = False\n        logging.warn(\"Unsupport fuse_layers featrue for CPU device with QBits backend!\")\n    # Prepare WQLinear layers, replace nn.Linear\n    self._load_quantized_modules(\n        self,\n        model,\n        quant_config,\n        quant_config.version,\n        use_exllama=use_exllama,\n        use_exllama_v2=use_exllama_v2,\n        use_qbits=use_cpu_qbits,\n    )\n\n    model.tie_weights()\n\n    # loads the weights into modules and distributes\n    # across available devices automatically\n    load_checkpoint_and_dispatch(\n        model,\n        checkpoint=model_weights_path,\n        device_map=device_map,\n        max_memory=max_memory,\n        no_split_module_classes=[self.layer_type],\n        offload_folder=offload_folder,\n        dtype=torch_dtype,\n    )\n\n    # Dispath to devices\n    if fuse_layers:\n        self.fuse_layers(model)\n\n    if use_cpu_qbits:\n        dtype = torch.bfloat16 if check_isa_supported(\"AMX\") else torch.float32\n        model.to(dtype=dtype, device=\"cpu\")\n        # repack qweight to match the QBits kernel.\n        model = qbits_post_init(model)\n    elif quant_config.version == \"marlin\":\n        model = marlin_post_init(model)\n    elif use_exllama:\n        # creates q4 handle\n        model = exllama_post_init(model)\n    elif use_exllama_v2:\n        # creates q4 handle and allocates scratch spaces wrt max_input_len and max_batch_size\n        model = exllamav2_post_init(\n            model,\n            max_input_len=max_seq_len or 2048,\n            max_batch_size=int(os.getenv(\"AWQ_BATCH_SIZE\", 1)),\n        )\n\n    return self(\n        model,\n        model_type,\n        is_quantized=True,\n        config=config,\n        quant_config=quant_config,\n        processor=None,\n    )\n</code></pre>"}]}